{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: theano in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from theano) (1.15.4)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from theano) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from theano) (1.11.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git\n",
    "# Theano is an open source numeric computation library, very efficient and based on numpy syntax.  Theano can not only run on \n",
    "# your CPU (Central Processing Unit) but also on your GPU (Graphical Processing Unit)\n",
    "!pip install theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: keras in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.15.4)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000016DE3A5F710>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')': /simple/keras/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000016DE3A5FBE0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')': /simple/keras/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000016DE3A5F2B0>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')': /simple/keras/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000016DE3A5F438>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')': /simple/keras/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x0000016DE3A5F748>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it')': /simple/keras/\n"
     ]
    }
   ],
   "source": [
    "# Installing Tensorflow\n",
    "# Install Tensorflow from the website: https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html\n",
    "# Tensorflow is another open source numeric computation library, runs very fast computation and can run either on your CPU or GPU\n",
    "# If you are using Theano and Tensorflow together it means that you are building a deep neural network FROM SCRATCH\n",
    "#!pip install tensorflow\n",
    "# However if you want to wrap Theano and Tensorflow together then you can directly use Keras.  This library allows us to build a\n",
    "# deep neural network using a few lines of code.  Keras is extremely powerful and runs on Theano and Tensorflow libraries. \n",
    "# Installing Keras\n",
    "!pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "#Since the first column is considered column \"0\" and we need columns from 3 to 12 - we put 3:13 (the upper bound 13 is excluded)\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "#The label or dependent variable is the 13th variable that is considered\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>15574012</td>\n",
       "      <td>Chu</td>\n",
       "      <td>645</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>113755.78</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>149756.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>15592531</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>822</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10062.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>15656148</td>\n",
       "      <td>Obinna</td>\n",
       "      <td>376</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>115046.74</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119346.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>15792365</td>\n",
       "      <td>He</td>\n",
       "      <td>501</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>142051.07</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>74940.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>15592389</td>\n",
       "      <td>H?</td>\n",
       "      <td>684</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>134603.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71725.73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>15767821</td>\n",
       "      <td>Bearce</td>\n",
       "      <td>528</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>102016.72</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>80181.12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>15737173</td>\n",
       "      <td>Andrews</td>\n",
       "      <td>497</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76390.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>15632264</td>\n",
       "      <td>Kay</td>\n",
       "      <td>476</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>26260.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>15691483</td>\n",
       "      <td>Chin</td>\n",
       "      <td>549</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>190857.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15600882</td>\n",
       "      <td>Scott</td>\n",
       "      <td>635</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65951.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>15643966</td>\n",
       "      <td>Goforth</td>\n",
       "      <td>616</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>143129.41</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64327.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>15737452</td>\n",
       "      <td>Romeo</td>\n",
       "      <td>653</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>132602.88</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5097.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>15788218</td>\n",
       "      <td>Henderson</td>\n",
       "      <td>549</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14406.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>15661507</td>\n",
       "      <td>Muldrow</td>\n",
       "      <td>587</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158684.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>15568982</td>\n",
       "      <td>Hao</td>\n",
       "      <td>726</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54724.03</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>15577657</td>\n",
       "      <td>McDonald</td>\n",
       "      <td>732</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170886.17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>15597945</td>\n",
       "      <td>Dellucci</td>\n",
       "      <td>636</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>138555.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>15699309</td>\n",
       "      <td>Gerasimov</td>\n",
       "      <td>510</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>118913.53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>15725737</td>\n",
       "      <td>Mosman</td>\n",
       "      <td>669</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8487.75</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>15625047</td>\n",
       "      <td>Yen</td>\n",
       "      <td>846</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>187616.16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>15738191</td>\n",
       "      <td>Maclean</td>\n",
       "      <td>577</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>124508.29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>15736816</td>\n",
       "      <td>Young</td>\n",
       "      <td>756</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>136815.64</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>170041.95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>15700772</td>\n",
       "      <td>Nebechi</td>\n",
       "      <td>571</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38433.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>15728693</td>\n",
       "      <td>McWilliams</td>\n",
       "      <td>574</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>141349.43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100187.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>15656300</td>\n",
       "      <td>Lucciano</td>\n",
       "      <td>411</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>59697.17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53483.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>9971</td>\n",
       "      <td>15587133</td>\n",
       "      <td>Thompson</td>\n",
       "      <td>518</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>7</td>\n",
       "      <td>151027.05</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119377.36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>9972</td>\n",
       "      <td>15721377</td>\n",
       "      <td>Chou</td>\n",
       "      <td>833</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>144751.81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166472.81</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>9973</td>\n",
       "      <td>15747927</td>\n",
       "      <td>Ch'in</td>\n",
       "      <td>758</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>155739.76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>171552.02</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>9974</td>\n",
       "      <td>15806455</td>\n",
       "      <td>Miller</td>\n",
       "      <td>611</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>157474.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>9975</td>\n",
       "      <td>15695474</td>\n",
       "      <td>Barker</td>\n",
       "      <td>583</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>122531.86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13549.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>9976</td>\n",
       "      <td>15666295</td>\n",
       "      <td>Smith</td>\n",
       "      <td>610</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>113957.01</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>196526.55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>9977</td>\n",
       "      <td>15656062</td>\n",
       "      <td>Azikiwe</td>\n",
       "      <td>637</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>103377.81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84419.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>9978</td>\n",
       "      <td>15579969</td>\n",
       "      <td>Mancini</td>\n",
       "      <td>683</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24991.92</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>9979</td>\n",
       "      <td>15703563</td>\n",
       "      <td>P'eng</td>\n",
       "      <td>774</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>93017.47</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>191608.97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>9980</td>\n",
       "      <td>15692664</td>\n",
       "      <td>Diribe</td>\n",
       "      <td>677</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>90022.85</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2988.28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>9981</td>\n",
       "      <td>15719276</td>\n",
       "      <td>T'ao</td>\n",
       "      <td>741</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>74371.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99595.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>9982</td>\n",
       "      <td>15672754</td>\n",
       "      <td>Burbidge</td>\n",
       "      <td>498</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>152039.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>53445.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>9983</td>\n",
       "      <td>15768163</td>\n",
       "      <td>Griffin</td>\n",
       "      <td>655</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>137145.12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>115146.40</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>9984</td>\n",
       "      <td>15656710</td>\n",
       "      <td>Cocci</td>\n",
       "      <td>613</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151325.24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>9985</td>\n",
       "      <td>15696175</td>\n",
       "      <td>Echezonachukwu</td>\n",
       "      <td>602</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>90602.42</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>51695.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>9986</td>\n",
       "      <td>15586914</td>\n",
       "      <td>Nepean</td>\n",
       "      <td>659</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>6</td>\n",
       "      <td>123841.49</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96833.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>9987</td>\n",
       "      <td>15581736</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>673</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>183579.54</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34047.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>9988</td>\n",
       "      <td>15588839</td>\n",
       "      <td>Mancini</td>\n",
       "      <td>606</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>180307.73</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1914.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>9989</td>\n",
       "      <td>15589329</td>\n",
       "      <td>Pirozzi</td>\n",
       "      <td>775</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>49337.84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>9990</td>\n",
       "      <td>15605622</td>\n",
       "      <td>McMillan</td>\n",
       "      <td>841</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>179436.60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>9991</td>\n",
       "      <td>15798964</td>\n",
       "      <td>Nkemakonam</td>\n",
       "      <td>714</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>35016.60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53667.08</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>9992</td>\n",
       "      <td>15769959</td>\n",
       "      <td>Ajuluchukwu</td>\n",
       "      <td>597</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>88381.21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69384.71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>9993</td>\n",
       "      <td>15657105</td>\n",
       "      <td>Chukwualuka</td>\n",
       "      <td>726</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>195192.40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>9994</td>\n",
       "      <td>15569266</td>\n",
       "      <td>Rahman</td>\n",
       "      <td>644</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>155060.41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29179.52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>9995</td>\n",
       "      <td>15719294</td>\n",
       "      <td>Wood</td>\n",
       "      <td>800</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167773.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9996</td>\n",
       "      <td>15606229</td>\n",
       "      <td>Obijiaku</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9997</td>\n",
       "      <td>15569892</td>\n",
       "      <td>Johnstone</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9998</td>\n",
       "      <td>15584532</td>\n",
       "      <td>Liu</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9999</td>\n",
       "      <td>15682355</td>\n",
       "      <td>Sabbatini</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>10000</td>\n",
       "      <td>15628319</td>\n",
       "      <td>Walker</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RowNumber  CustomerId         Surname  CreditScore Geography  Gender  \\\n",
       "0             1    15634602        Hargrave          619    France  Female   \n",
       "1             2    15647311            Hill          608     Spain  Female   \n",
       "2             3    15619304            Onio          502    France  Female   \n",
       "3             4    15701354            Boni          699    France  Female   \n",
       "4             5    15737888        Mitchell          850     Spain  Female   \n",
       "5             6    15574012             Chu          645     Spain    Male   \n",
       "6             7    15592531        Bartlett          822    France    Male   \n",
       "7             8    15656148          Obinna          376   Germany  Female   \n",
       "8             9    15792365              He          501    France    Male   \n",
       "9            10    15592389              H?          684    France    Male   \n",
       "10           11    15767821          Bearce          528    France    Male   \n",
       "11           12    15737173         Andrews          497     Spain    Male   \n",
       "12           13    15632264             Kay          476    France  Female   \n",
       "13           14    15691483            Chin          549    France  Female   \n",
       "14           15    15600882           Scott          635     Spain  Female   \n",
       "15           16    15643966         Goforth          616   Germany    Male   \n",
       "16           17    15737452           Romeo          653   Germany    Male   \n",
       "17           18    15788218       Henderson          549     Spain  Female   \n",
       "18           19    15661507         Muldrow          587     Spain    Male   \n",
       "19           20    15568982             Hao          726    France  Female   \n",
       "20           21    15577657        McDonald          732    France    Male   \n",
       "21           22    15597945        Dellucci          636     Spain  Female   \n",
       "22           23    15699309       Gerasimov          510     Spain  Female   \n",
       "23           24    15725737          Mosman          669    France    Male   \n",
       "24           25    15625047             Yen          846    France  Female   \n",
       "25           26    15738191         Maclean          577    France    Male   \n",
       "26           27    15736816           Young          756   Germany    Male   \n",
       "27           28    15700772         Nebechi          571    France    Male   \n",
       "28           29    15728693      McWilliams          574   Germany  Female   \n",
       "29           30    15656300        Lucciano          411    France    Male   \n",
       "...         ...         ...             ...          ...       ...     ...   \n",
       "9970       9971    15587133        Thompson          518    France    Male   \n",
       "9971       9972    15721377            Chou          833    France  Female   \n",
       "9972       9973    15747927           Ch'in          758    France    Male   \n",
       "9973       9974    15806455          Miller          611    France    Male   \n",
       "9974       9975    15695474          Barker          583    France    Male   \n",
       "9975       9976    15666295           Smith          610   Germany    Male   \n",
       "9976       9977    15656062         Azikiwe          637    France  Female   \n",
       "9977       9978    15579969         Mancini          683    France  Female   \n",
       "9978       9979    15703563           P'eng          774    France    Male   \n",
       "9979       9980    15692664          Diribe          677    France  Female   \n",
       "9980       9981    15719276            T'ao          741     Spain    Male   \n",
       "9981       9982    15672754        Burbidge          498   Germany    Male   \n",
       "9982       9983    15768163         Griffin          655   Germany  Female   \n",
       "9983       9984    15656710           Cocci          613    France    Male   \n",
       "9984       9985    15696175  Echezonachukwu          602   Germany    Male   \n",
       "9985       9986    15586914          Nepean          659    France    Male   \n",
       "9986       9987    15581736        Bartlett          673   Germany    Male   \n",
       "9987       9988    15588839         Mancini          606     Spain    Male   \n",
       "9988       9989    15589329         Pirozzi          775    France    Male   \n",
       "9989       9990    15605622        McMillan          841     Spain    Male   \n",
       "9990       9991    15798964      Nkemakonam          714   Germany    Male   \n",
       "9991       9992    15769959     Ajuluchukwu          597    France  Female   \n",
       "9992       9993    15657105     Chukwualuka          726     Spain    Male   \n",
       "9993       9994    15569266          Rahman          644    France    Male   \n",
       "9994       9995    15719294            Wood          800    France  Female   \n",
       "9995       9996    15606229        Obijiaku          771    France    Male   \n",
       "9996       9997    15569892       Johnstone          516    France    Male   \n",
       "9997       9998    15584532             Liu          709    France  Female   \n",
       "9998       9999    15682355       Sabbatini          772   Germany    Male   \n",
       "9999      10000    15628319          Walker          792    France  Female   \n",
       "\n",
       "      Age  Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0      42       2       0.00              1          1               1   \n",
       "1      41       1   83807.86              1          0               1   \n",
       "2      42       8  159660.80              3          1               0   \n",
       "3      39       1       0.00              2          0               0   \n",
       "4      43       2  125510.82              1          1               1   \n",
       "5      44       8  113755.78              2          1               0   \n",
       "6      50       7       0.00              2          1               1   \n",
       "7      29       4  115046.74              4          1               0   \n",
       "8      44       4  142051.07              2          0               1   \n",
       "9      27       2  134603.88              1          1               1   \n",
       "10     31       6  102016.72              2          0               0   \n",
       "11     24       3       0.00              2          1               0   \n",
       "12     34      10       0.00              2          1               0   \n",
       "13     25       5       0.00              2          0               0   \n",
       "14     35       7       0.00              2          1               1   \n",
       "15     45       3  143129.41              2          0               1   \n",
       "16     58       1  132602.88              1          1               0   \n",
       "17     24       9       0.00              2          1               1   \n",
       "18     45       6       0.00              1          0               0   \n",
       "19     24       6       0.00              2          1               1   \n",
       "20     41       8       0.00              2          1               1   \n",
       "21     32       8       0.00              2          1               0   \n",
       "22     38       4       0.00              1          1               0   \n",
       "23     46       3       0.00              2          0               1   \n",
       "24     38       5       0.00              1          1               1   \n",
       "25     25       3       0.00              2          0               1   \n",
       "26     36       2  136815.64              1          1               1   \n",
       "27     44       9       0.00              2          0               0   \n",
       "28     43       3  141349.43              1          1               1   \n",
       "29     29       0   59697.17              2          1               1   \n",
       "...   ...     ...        ...            ...        ...             ...   \n",
       "9970   42       7  151027.05              2          1               0   \n",
       "9971   34       3  144751.81              1          0               0   \n",
       "9972   26       4  155739.76              1          1               0   \n",
       "9973   27       7       0.00              2          1               1   \n",
       "9974   33       7  122531.86              1          1               0   \n",
       "9975   50       1  113957.01              2          1               0   \n",
       "9976   33       7  103377.81              1          1               0   \n",
       "9977   32       9       0.00              2          1               1   \n",
       "9978   40       9   93017.47              2          1               0   \n",
       "9979   58       1   90022.85              1          0               1   \n",
       "9980   35       6   74371.49              1          0               0   \n",
       "9981   42       3  152039.70              1          1               1   \n",
       "9982   46       7  137145.12              1          1               0   \n",
       "9983   40       4       0.00              1          0               0   \n",
       "9984   35       7   90602.42              2          1               1   \n",
       "9985   36       6  123841.49              2          1               0   \n",
       "9986   47       1  183579.54              2          0               1   \n",
       "9987   30       8  180307.73              2          1               1   \n",
       "9988   30       4       0.00              2          1               0   \n",
       "9989   28       4       0.00              2          1               1   \n",
       "9990   33       3   35016.60              1          1               0   \n",
       "9991   53       4   88381.21              1          1               0   \n",
       "9992   36       2       0.00              1          1               0   \n",
       "9993   28       7  155060.41              1          1               0   \n",
       "9994   29       2       0.00              2          0               0   \n",
       "9995   39       5       0.00              2          1               0   \n",
       "9996   35      10   57369.61              1          1               1   \n",
       "9997   36       7       0.00              1          0               1   \n",
       "9998   42       3   75075.31              2          1               0   \n",
       "9999   28       4  130142.79              1          1               0   \n",
       "\n",
       "      EstimatedSalary  Exited  \n",
       "0           101348.88       1  \n",
       "1           112542.58       0  \n",
       "2           113931.57       1  \n",
       "3            93826.63       0  \n",
       "4            79084.10       0  \n",
       "5           149756.71       1  \n",
       "6            10062.80       0  \n",
       "7           119346.88       1  \n",
       "8            74940.50       0  \n",
       "9            71725.73       0  \n",
       "10           80181.12       0  \n",
       "11           76390.01       0  \n",
       "12           26260.98       0  \n",
       "13          190857.79       0  \n",
       "14           65951.65       0  \n",
       "15           64327.26       0  \n",
       "16            5097.67       1  \n",
       "17           14406.41       0  \n",
       "18          158684.81       0  \n",
       "19           54724.03       0  \n",
       "20          170886.17       0  \n",
       "21          138555.46       0  \n",
       "22          118913.53       1  \n",
       "23            8487.75       0  \n",
       "24          187616.16       0  \n",
       "25          124508.29       0  \n",
       "26          170041.95       0  \n",
       "27           38433.35       0  \n",
       "28          100187.43       0  \n",
       "29           53483.21       0  \n",
       "...               ...     ...  \n",
       "9970        119377.36       0  \n",
       "9971        166472.81       0  \n",
       "9972        171552.02       0  \n",
       "9973        157474.10       0  \n",
       "9974         13549.24       0  \n",
       "9975        196526.55       1  \n",
       "9976         84419.78       0  \n",
       "9977         24991.92       0  \n",
       "9978        191608.97       0  \n",
       "9979          2988.28       0  \n",
       "9980         99595.67       0  \n",
       "9981         53445.17       1  \n",
       "9982        115146.40       1  \n",
       "9983        151325.24       0  \n",
       "9984         51695.41       0  \n",
       "9985         96833.00       0  \n",
       "9986         34047.54       0  \n",
       "9987          1914.41       0  \n",
       "9988         49337.84       0  \n",
       "9989        179436.60       0  \n",
       "9990         53667.08       0  \n",
       "9991         69384.71       1  \n",
       "9992        195192.40       0  \n",
       "9993         29179.52       0  \n",
       "9994        167773.55       0  \n",
       "9995         96270.64       0  \n",
       "9996        101699.77       0  \n",
       "9997         42085.58       1  \n",
       "9998         92888.52       1  \n",
       "9999         38190.78       0  \n",
       "\n",
       "[10000 rows x 14 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "#Here we are encoding the country and gender features below\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "#The country variable is again changed from label encoding to one hot encoding thereby introducing dummy variables\n",
    "#In the next line the [1] refers to the country variable which is the second from left.  First one being 0 and second one is 1\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "#The next line eliminates THE DUMMY VARIABLE TRAP by reducing one column.  Here whenever two columns have 0 it assumes it is the\n",
    "#third country\n",
    "X = X[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 0.0000000e+00, 6.1900000e+02, 0.0000000e+00,\n",
       "       4.2000000e+01, 2.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "       1.0000000e+00, 1.0000000e+00, 1.0134888e+05])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will take the first row where there is a credit score of 619.  As we can see from the result below - France corresponds to \n",
    "#zeroes in the first two columns, female is 0, if he has card then it is 1 else 0, is active member is 1 else 0.  \n",
    "#These are some transformations we will make a note of for now\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "#Feature scaling is very essential in deep learning.  It eliminates the value of one independent variable dominating the others\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5698444 ,  1.74309049,  0.16958176, ...,  0.64259497,\n",
       "        -1.03227043,  1.10643166],\n",
       "       [ 1.75486502, -0.57369368, -2.30455945, ...,  0.64259497,\n",
       "         0.9687384 , -0.74866447],\n",
       "       [-0.5698444 , -0.57369368, -1.19119591, ...,  0.64259497,\n",
       "        -1.03227043,  1.48533467],\n",
       "       ...,\n",
       "       [-0.5698444 , -0.57369368,  0.9015152 , ...,  0.64259497,\n",
       "        -1.03227043,  1.41231994],\n",
       "       [-0.5698444 ,  1.74309049, -0.62420521, ...,  0.64259497,\n",
       "         0.9687384 ,  0.84432121],\n",
       "       [ 1.75486502, -0.57369368, -0.28401079, ...,  0.64259497,\n",
       "        -1.03227043,  0.32472465]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Now let's make the ANN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "#The sequential library is used to initialize our neural network\n",
    "from keras.models import Sequential\n",
    "#The dense library is used to build the layers of our ANN\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Adding the input layer and the first hidden layer.  Here the rectifier activation function is used for the hidden layer. \n",
    "# input_dim refers to the number of features that feed as input. This is only mentioned for the first hidden layer\n",
    "# output_dim refers to the number of nodes of the first hidden layer.  Normally the number of nodes is decided as the average of\n",
    "# the number of points in the input layer (number of features) and the number of points in the output layer. (i+o)/2.  Here it\n",
    "# is 11+1=12/2=6\n",
    "# init refers to randomly initializing the weights in the first run of forward propogation in the stochastic gradient descent. \n",
    "# You do this initialization uniformly.  Two options are glorot_uniform and uniform. \n",
    "# The activation function relu refers to the rectifier activation function\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Adding the second hiddeclassifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))n layer.  Again the rectifier activation function is used for the hidden layer\n",
    "# As you notice the input_dim is missing as we are trying to create a second hidden layer - for which input layer is not required\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Adding the output layer.  Here the sigmoid activation function is used for the output layer as we are looking for probabilities\n",
    "# in this case the output_dim has a value of 1 as it refers to the predicted label field. \n",
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the ANN\n",
    "# Optimizer is the algorithm you would use to find the optimal set of weights.  There are different types of stochastic gradient\n",
    "# descent algorithms.  One of the most efficient one is the adam algorithm\n",
    "# Loss refers to the loss function within the stochastic gradient descent adam algorithm. If the predicted label has two outcomes\n",
    "# then the loss function used is binary_crossentropy.  If there are more than two then it is categorical_crossentropy.\n",
    "# For metrics here we are going to use accuracy as a measurement to see how well the algorithm is performing\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 7s 910us/step - loss: 0.4968 - acc: 0.7952\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 3s 398us/step - loss: 0.4284 - acc: 0.7960\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 3s 404us/step - loss: 0.4202 - acc: 0.8097\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 3s 336us/step - loss: 0.4137 - acc: 0.8296\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.4089 - acc: 0.8337\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.4056 - acc: 0.8335\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 286us/step - loss: 0.4031 - acc: 0.8345\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 3s 341us/step - loss: 0.4008 - acc: 0.8352 0s - loss: 0.4072 - acc\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 294us/step - loss: 0.4003 - acc: 0.8335\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 3s 314us/step - loss: 0.3986 - acc: 0.8342\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 3s 425us/step - loss: 0.3986 - acc: 0.8356\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 3s 390us/step - loss: 0.3973 - acc: 0.8354\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 3s 326us/step - loss: 0.3969 - acc: 0.8355\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 295us/step - loss: 0.3959 - acc: 0.8360\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 3s 318us/step - loss: 0.3951 - acc: 0.8371\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 242us/step - loss: 0.3932 - acc: 0.8365\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 255us/step - loss: 0.3919 - acc: 0.8384\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - ETA: 0s - loss: 0.3885 - acc: 0.838 - 2s 248us/step - loss: 0.3884 - acc: 0.8382\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 205us/step - loss: 0.3844 - acc: 0.8387\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 0.3802 - acc: 0.8404\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 0.3747 - acc: 0.8424\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3696 - acc: 0.8455\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.3639 - acc: 0.8470 0s - loss: 0.\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 166us/step - loss: 0.3595 - acc: 0.8505\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 0.3545 - acc: 0.8534\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.3526 - acc: 0.8557 0s - loss: 0.3588 - \n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.3506 - acc: 0.8539 1s - loss: 0.3293  - ETA: 0s - loss: 0.3557 - a\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.3488 - acc: 0.8554\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.3473 - acc: 0.8584\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3467 - acc: 0.8582\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.3453 - acc: 0.8560 1s - loss: 0\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.3446 - acc: 0.8574\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.3443 - acc: 0.8576\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 170us/step - loss: 0.3427 - acc: 0.8601\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 171us/step - loss: 0.3433 - acc: 0.8594\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3433 - acc: 0.8575\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.3422 - acc: 0.8587\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3415 - acc: 0.8575\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 259us/step - loss: 0.3420 - acc: 0.8599\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.3417 - acc: 0.8590\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3414 - acc: 0.8586\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 0.3407 - acc: 0.8597\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3409 - acc: 0.8616\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 230us/step - loss: 0.3418 - acc: 0.8591\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.3410 - acc: 0.8601\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 249us/step - loss: 0.3402 - acc: 0.8602\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 226us/step - loss: 0.3415 - acc: 0.8582\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 219us/step - loss: 0.3404 - acc: 0.8597 0s - loss: 0.3\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 200us/step - loss: 0.3399 - acc: 0.8597 0s - loss: 0.3403 - acc: 0.860\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 0.3406 - acc: 0.8594\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 246us/step - loss: 0.3402 - acc: 0.8614\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 0.3400 - acc: 0.8570\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 186us/step - loss: 0.3405 - acc: 0.8580\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.3384 - acc: 0.8589\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3388 - acc: 0.8612\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.3387 - acc: 0.8596 1s - lo\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 0.3399 - acc: 0.8592 0s - loss: 0.3429 - ac\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.3400 - acc: 0.8601\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.3404 - acc: 0.8597\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.3390 - acc: 0.8595\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 0.3385 - acc: 0.8586\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.3386 - acc: 0.8609\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.3379 - acc: 0.8594 0s - loss: 0.3294 -\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.3383 - acc: 0.8582\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 234us/step - loss: 0.3385 - acc: 0.8574\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.3384 - acc: 0.8592\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.3382 - acc: 0.8594\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 264us/step - loss: 0.3389 - acc: 0.8597\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 248us/step - loss: 0.3366 - acc: 0.8622\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 210us/step - loss: 0.3369 - acc: 0.8596\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.3389 - acc: 0.8594\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.3384 - acc: 0.8585\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 0.3369 - acc: 0.8585 0s - loss: 0.3427 - acc: 0\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 168us/step - loss: 0.3377 - acc: 0.8595\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 182us/step - loss: 0.3375 - acc: 0.8614 0s - loss: 0.3383 - a\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 0.3383 - acc: 0.8627\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 227us/step - loss: 0.3363 - acc: 0.8617\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3373 - acc: 0.8634\n",
      "Epoch 79/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 1s 141us/step - loss: 0.3377 - acc: 0.8611\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 0.3372 - acc: 0.8615\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.3362 - acc: 0.8606 0s - loss: 0.327\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.3373 - acc: 0.8609\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.3379 - acc: 0.8577\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 245us/step - loss: 0.3385 - acc: 0.8592\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.3368 - acc: 0.8611 0s - loss: 0.3382 - acc: 0.8\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.3376 - acc: 0.8597\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 217us/step - loss: 0.3357 - acc: 0.8620\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 0.3362 - acc: 0.8609\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 194us/step - loss: 0.3365 - acc: 0.8636\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.3375 - acc: 0.8617\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.3370 - acc: 0.8620\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 239us/step - loss: 0.3370 - acc: 0.8595\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.3353 - acc: 0.8621\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 220us/step - loss: 0.3365 - acc: 0.8611\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 212us/step - loss: 0.3366 - acc: 0.8605\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 206us/step - loss: 0.3364 - acc: 0.8632\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 213us/step - loss: 0.3361 - acc: 0.8620 \n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.3355 - acc: 0.8609\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.3364 - acc: 0.8592\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 188us/step - loss: 0.3359 - acc: 0.8644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dae9bb4208>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the ANN to the Training set\n",
    "# batch_size refers to the number of rows/observations that you want to consider for each run when calibrating the weights\n",
    "# nb_epoch refers to the number of times the ann would be iterated\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1501,   94],\n",
       "       [ 190,  215]], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model on a single customer to predict for churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now let us make a prediction for a single row wether a customer will churn for the following values:\n",
    "Geography: France\n",
    "Credit Score: 600\n",
    "Gender: Male\n",
    "Age: 40 years old\n",
    "Tenure: 3 years\n",
    "Balance: $60000\n",
    "Number of Products: 2\n",
    "Does this customer have a credit card ? Yes\n",
    "Is this customer an Active Member: Yes\n",
    "Estimated Salary: $50000\n",
    "So should we say goodbye to that customer ?\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now we have to see how the encodings and transformations have happened while we apply the above values.  For this let us compare\n",
    "the original dataset with the tranformed array of X\"\"\"\n",
    "#As we saw earlier - France corresponds to zeroes in the first two columns, male is 1, He has a credit card which gives a value\n",
    "#of 1, is an active member - which again takes a value of 1.  So the above data should be like this:\n",
    "# 0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#It is important that we pass this data as a numpy array and it is VERY IMPORTANT TO PUT IT INTO TWO SQUARE BRACKETS.  If you\n",
    "#put one set of square brackets - it will consider it as column headers.  If you put two square brackets - it will take it as\n",
    "#a 2 dimensional array.  \n",
    "#Also since we have done scaling - we use the same sc method to transform the array - before passing it into prediction\n",
    "new_prediction = classifier.predict(sc.transform(np.array([[0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])))\n",
    "#The below code will change the prediction into True or False\n",
    "new_prediction = (new_prediction > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement K-fold cross validation to fine tune model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#The sequential library is used to initialize our neural network\n",
    "from keras.models import Sequential\n",
    "#The dense library is used to build the layers of our ANN\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, nb_epoch = 100)\n",
    "#In the next line we are doing the cross validation.  Here cv = 10 means that we are doing for 10 folds & n_jobs = -1 means that\n",
    "#we are using all the CPU capacity to get this job done.  This will take a long time to execute.\n",
    "#accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 2, n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the ANN performance\n",
    "## Dropout Regularization to reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  if sys.path[0] == '':\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.1)`\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "#Dropout Regularization to improve overfitting if needed\n",
    "#This technique works this way : at each iteration of your artificial neural network - some neurons are randomly disabled to\n",
    "#prevent them from being too dependent on each other(when they learn the correlations).  This way the neurons learn several\n",
    "#independant correlations in the data because each time there is not the same configuration of the neurons.  The fact that we get\n",
    "#these independant correlations of the data because now the neurons are working more independently : prevents the neurons from\n",
    "#learning too much and therefore prevents overfitting\n",
    "from keras.layers import Dropout\n",
    "#You should apply dropout to all the layers. Adding dropout to first layers in next two steps\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "classifier.add(Dropout(p = 0.1))\n",
    "#Adding droput to second layer in next two steps\n",
    "classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "classifier.add(Dropout(p = 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 2s 275us/step - loss: 0.4872 - acc: 0.7959 0s - loss: 0.5114 - acc: \n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 0.4358 - acc: 0.7960 0s - loss: 0.4409 - \n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 152us/step - loss: 0.4312 - acc: 0.7960\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.4278 - acc: 0.7960\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 160us/step - loss: 0.4250 - acc: 0.8112\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 157us/step - loss: 0.4254 - acc: 0.8234\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 189us/step - loss: 0.4252 - acc: 0.8260 0s - loss: 0.4243 - acc: 0.\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.4232 - acc: 0.8250\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 286us/step - loss: 0.4246 - acc: 0.8296\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 169us/step - loss: 0.4211 - acc: 0.8317\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 174us/step - loss: 0.4240 - acc: 0.8297 2s - loss: 0.4182 - acc: 0.82 - ETA: 1s - loss\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.4222 - acc: 0.8300\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 190us/step - loss: 0.4197 - acc: 0.8307\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 240us/step - loss: 0.4225 - acc: 0.8317\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.4204 - acc: 0.8340\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 0.4221 - acc: 0.8304\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 303us/step - loss: 0.4224 - acc: 0.8309\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 293us/step - loss: 0.4239 - acc: 0.8310\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 262us/step - loss: 0.4205 - acc: 0.8307\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 277us/step - loss: 0.4222 - acc: 0.8309\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 298us/step - loss: 0.4186 - acc: 0.8316\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 244us/step - loss: 0.4208 - acc: 0.8317\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 3s 318us/step - loss: 0.4224 - acc: 0.8309\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 3s 387us/step - loss: 0.4197 - acc: 0.8324\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 3s 362us/step - loss: 0.4205 - acc: 0.8306\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 274us/step - loss: 0.4228 - acc: 0.8317 0s - loss: 0.4242 - acc\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 286us/step - loss: 0.4205 - acc: 0.8301\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 280us/step - loss: 0.4177 - acc: 0.8315 1s - lo\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 237us/step - loss: 0.4202 - acc: 0.8311\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 265us/step - loss: 0.4205 - acc: 0.8322\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.4185 - acc: 0.8329\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 0.4217 - acc: 0.8334 1s - loss:\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 198us/step - loss: 0.4194 - acc: 0.8320 1s -\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 203us/step - loss: 0.4201 - acc: 0.8329\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 254us/step - loss: 0.4219 - acc: 0.8300 1s - \n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 185us/step - loss: 0.4207 - acc: 0.8307\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 176us/step - loss: 0.4224 - acc: 0.8307 1s - loss: \n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 175us/step - loss: 0.4208 - acc: 0.8315\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.4211 - acc: 0.8309 0s - loss: 0.4221 - acc: 0.831\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 197us/step - loss: 0.4208 - acc: 0.8305\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 290us/step - loss: 0.4200 - acc: 0.8317\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 241us/step - loss: 0.4208 - acc: 0.8327\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 184us/step - loss: 0.4172 - acc: 0.8324\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 0.4213 - acc: 0.8327\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.4223 - acc: 0.8311 0s - loss: 0.4302 - a\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 183us/step - loss: 0.4186 - acc: 0.8334\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 215us/step - loss: 0.4182 - acc: 0.8311\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.4183 - acc: 0.8336\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 202us/step - loss: 0.4216 - acc: 0.8336\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 195us/step - loss: 0.4190 - acc: 0.8295\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 207us/step - loss: 0.4198 - acc: 0.8334\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 232us/step - loss: 0.4194 - acc: 0.8344\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 307us/step - loss: 0.4191 - acc: 0.8329\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 214us/step - loss: 0.4169 - acc: 0.8326\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 201us/step - loss: 0.4216 - acc: 0.8326\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 307us/step - loss: 0.4204 - acc: 0.8349\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 307us/step - loss: 0.4216 - acc: 0.8325\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 299us/step - loss: 0.4174 - acc: 0.8327\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 288us/step - loss: 0.4191 - acc: 0.8327\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 308us/step - loss: 0.4185 - acc: 0.8335\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 289us/step - loss: 0.4162 - acc: 0.8320\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 3s 326us/step - loss: 0.4213 - acc: 0.8331\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 3s 337us/step - loss: 0.4188 - acc: 0.8332\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 283us/step - loss: 0.4198 - acc: 0.8306\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 256us/step - loss: 0.4197 - acc: 0.8339\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 3s 318us/step - loss: 0.4168 - acc: 0.8317\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 279us/step - loss: 0.4202 - acc: 0.8339\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.4184 - acc: 0.8342\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 303us/step - loss: 0.4197 - acc: 0.8315\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 286us/step - loss: 0.4211 - acc: 0.8335\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 273us/step - loss: 0.4205 - acc: 0.8322\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 3s 328us/step - loss: 0.4203 - acc: 0.8325\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 283us/step - loss: 0.4191 - acc: 0.8321\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 4s 449us/step - loss: 0.4171 - acc: 0.8340\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 3s 393us/step - loss: 0.4181 - acc: 0.8319\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 3s 328us/step - loss: 0.4197 - acc: 0.8321\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.4195 - acc: 0.8337\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 209us/step - loss: 0.4216 - acc: 0.8339\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 161us/step - loss: 0.4166 - acc: 0.8315\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 164us/step - loss: 0.4163 - acc: 0.8346 1s - loss: 0\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.4173 - acc: 0.8332\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 181us/step - loss: 0.4175 - acc: 0.8351\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 211us/step - loss: 0.4209 - acc: 0.8325\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.4210 - acc: 0.8320\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 199us/step - loss: 0.4159 - acc: 0.8340 1s - loss:\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4184 - acc: 0.8351\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 196us/step - loss: 0.4207 - acc: 0.8345\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 221us/step - loss: 0.4197 - acc: 0.8342\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 223us/step - loss: 0.4183 - acc: 0.8316\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 228us/step - loss: 0.4181 - acc: 0.8362\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 208us/step - loss: 0.4228 - acc: 0.8335\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 187us/step - loss: 0.4194 - acc: 0.8331\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 218us/step - loss: 0.4208 - acc: 0.8331\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 238us/step - loss: 0.4189 - acc: 0.8334\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 297us/step - loss: 0.4205 - acc: 0.8310 1s \n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 257us/step - loss: 0.4216 - acc: 0.8341\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 222us/step - loss: 0.4190 - acc: 0.8327 1s - lo\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 225us/step - loss: 0.4170 - acc: 0.8307\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 204us/step - loss: 0.4184 - acc: 0.8336\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 236us/step - loss: 0.4200 - acc: 0.8304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x283895c8b70>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "classifier.fit(X_train, y_train, batch_size = 10, nb_epoch = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1556,   39],\n",
       "       [ 279,  126]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the ANN performance\n",
    "## Grid Search to improve accuracy with hyper parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "#The sequential library is used to initialize our neural network\n",
    "from keras.models import Sequential\n",
    "#The dense library is used to build the layers of our ANN\n",
    "from keras.layers import Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", input_dim=11, units=6, kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=6, kernel_initializer=\"uniform\")`\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1, kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 687us/step - loss: 0.5849 - acc: 0.7954\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 672us/step - loss: 0.5585 - acc: 0.7956\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 676us/step - loss: 0.5626 - acc: 0.7950\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 688us/step - loss: 0.5588 - acc: 0.7964\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 796us/step - loss: 0.5799 - acc: 0.7921\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 708us/step - loss: 0.5582 - acc: 0.7940\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 812us/step - loss: 0.5504 - acc: 0.7971\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 869us/step - loss: 0.5654 - acc: 0.7950\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 752us/step - loss: 0.5627 - acc: 0.7939\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 743us/step - loss: 0.5779 - acc: 0.7951\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 5s 731us/step - loss: 0.5822 - acc: 0.7967\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 813us/step - loss: 0.5693 - acc: 0.7950\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 848us/step - loss: 0.6173 - acc: 0.7936\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 798us/step - loss: 0.5630 - acc: 0.7964\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 839us/step - loss: 0.5683 - acc: 0.7932\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 853us/step - loss: 0.5646 - acc: 0.7938\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 805us/step - loss: 0.5865 - acc: 0.7956\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 790us/step - loss: 0.5796 - acc: 0.7954\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 800us/step - loss: 0.5778 - acc: 0.7946\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 842us/step - loss: 0.5789 - acc: 0.7947\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 810us/step - loss: 0.5508 - acc: 0.7971\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 962us/step - loss: 0.6041 - acc: 0.7940\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5643 - acc: 0.7943\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 963us/step - loss: 0.5498 - acc: 0.7944\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5647 - acc: 0.7924\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 18s 2ms/step - loss: 0.5493 - acc: 0.7944:\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 930us/step - loss: 0.5561 - acc: 0.7953\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 948us/step - loss: 0.5505 - acc: 0.7960\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 6s 899us/step - loss: 0.5471 - acc: 0.7949\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 909us/step - loss: 0.5576 - acc: 0.7954\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 908us/step - loss: 0.5650 - acc: 0.7971\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 979us/step - loss: 0.5527 - acc: 0.7967\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.5983 - acc: 0.7932\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5660 - acc: 0.7968\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 935us/step - loss: 0.5769 - acc: 0.7933\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 955us/step - loss: 0.6026 - acc: 0.7929\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 960us/step - loss: 0.5839 - acc: 0.7951\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.6109 - acc: 0.7954\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 943us/step - loss: 0.5826 - acc: 0.7942\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 1ms/step - loss: 0.5926 - acc: 0.7937\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 978us/step - loss: 0.5935 - acc: 0.7958\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.6065 - acc: 0.7951\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5817 - acc: 0.7956\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - ETA: 0s - loss: 0.6251 - acc: 0.795 - 8s 1ms/step - loss: 0.6220 - acc: 0.7956\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.5900 - acc: 0.7933\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 7s 1ms/step - loss: 0.5783 - acc: 0.7943\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5712 - acc: 0.7969\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5979 - acc: 0.7935\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.5818 - acc: 0.7951\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.5808 - acc: 0.7960\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.6304 - acc: 0.7968\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.5913 - acc: 0.7969\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.6415 - acc: 0.7932\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.5958 - acc: 0.7971\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.6002 - acc: 0.7922\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.6005 - acc: 0.7929\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.6137 - acc: 0.7951\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 8s 1ms/step - loss: 0.6404 - acc: 0.7939\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 9s 1ms/step - loss: 0.6115 - acc: 0.7951\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 10s 1ms/step - loss: 0.6136 - acc: 0.7932\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 18s 3ms/step - loss: 0.5836 - acc: 0.7951\n",
      "Epoch 1/1\n",
      "7200/7200 [==============================] - 12s 2ms/step - loss: 0.5903 - acc: 0.7951\n"
     ]
    }
   ],
   "source": [
    "'''Here you can see that batch_size and nb_epoch are part of the classifier hyper parameters, whereas the optimizer is an \n",
    "architecture parameter.  If we want to fine tune the architecture parameter as well like optimizer, loss etc. - then we would\n",
    "have to mention them when defining the build_classifier class as a parameter within brackets like this: \n",
    "def build_classifier(optimizer, loss).  In our case below I have chosen only to fine tune the optimizer only for now'''\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(output_dim = 6, init = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(output_dim = 1, init = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "#Next Set of lines are specific to grid search specification\n",
    "parameters = {'batch_size': [25, 32],\n",
    "              'nb_epoch': [100, 500],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = classifier, \n",
    "                           param_grid = parameters, \n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 25, 'nb_epoch': 100, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.796"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = grid_search.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# Making the Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
